{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "# Scaled Dot Product Attention\n",
    "\n",
    "class Attention(nn.Module) :\n",
    "    \n",
    "    def forward(self, query, key, value, mask = None, dropout = None) :\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.size(-1))\n",
    "        \n",
    "        if mask is not None :\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            \n",
    "        p_attn = F.softmax(scores, dim = 1)\n",
    "        \n",
    "        if dropout is not None :\n",
    "            p_attn = dropout(p_attn)\n",
    "            \n",
    "        return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\"\"\"\n",
    "multi_head.py\n",
    "\"\"\"\n",
    "\n",
    "class MultiHeadAttention(nn.Module) :\n",
    "    \n",
    "    def __init__(self, h, d_model, dropout = 0.1) :\n",
    "        super().__init()\n",
    "        assert d_model % h == 0\n",
    "        \n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        \n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "        self.attention = Attention()\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask = None) :\n",
    "        batch_size = query.size()\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1,2) for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "        \n",
    "        x, attn = self.attention(query, key, value, mask = mask, dropout = self.dropout)\n",
    "        \n",
    "        x = x.transpose(1,2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "        \n",
    "        return self.output_linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "transformer.py\n",
    "\"\"\"\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional Encoder = Transformer (self-attention)\n",
    "    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\n",
    "        \"\"\"\n",
    "        :param hidden: hidden size of transformer\n",
    "        :param attn_heads: head sizes of multi-head attention\n",
    "        :param feed_forward_hidden: feed_forward_hidden, usually 4*hidden_size\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\n",
    "        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n",
    "        x = self.output_sublayer(x, self.feed_forward)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\"\"\"\n",
    "segment.py\n",
    "\"\"\"\n",
    "\n",
    "class TokenEmbedding(nn.Embedding) :\n",
    "    def __init__(self, vocab_size, embed_size = 512) :\n",
    "        super().__init__(vocab_size, embed_size, padding_idx = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\"\"\"\n",
    "segment.py\n",
    "\"\"\"\n",
    "\n",
    "class SegmentEmbedding(nn.Embedding) :\n",
    "    \n",
    "    def __init__(self, embed_size = 512) :\n",
    "        super().__init__(3, embed_size, padding_idx = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\"\"\"\n",
    "position.py\n",
    "\"\"\"\n",
    "\n",
    "class PositionalEmbedding(nn.Module) :\n",
    "    \n",
    "    def __init__(self, d_model, max_len = 512) :\n",
    "        super().__init__()\n",
    "        \n",
    "        # compute positional encoding in log space\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.required_grad = False\n",
    "        \n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        return self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "bert.py\n",
    "\"\"\"\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, dropout=0.1):\n",
    "   \n",
    "        super().__init__()\n",
    "        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)\n",
    "        self.position = PositionalEmbedding(d_model=self.token.embedding_dim)\n",
    "        self.segment = SegmentEmbedding(embed_size=self.token.embedding_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, sequence, segment_label):\n",
    "        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "bert.py\n",
    "\"\"\"\n",
    "\n",
    "class BERT(nn.Module) :\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden = 768, n_layers = 12, attn_heads = 12, dropout = 0.1) :\n",
    "        super().__init__()\n",
    "        self.hidden = hidden\n",
    "        self.n_layer = n_layers\n",
    "        self.attn_heads = attn_heads\n",
    "        \n",
    "        # paper : use 4*hidden_size for ff network hidden size\n",
    "        self.feed_forward_hidden = hidden * 4\n",
    "        \n",
    "        # embedding for BERT = token + segment + position\n",
    "        self.embedding = BERTEmbedding(vocab_size = vocab_size, embed_size = hidden)\n",
    "        \n",
    "        # transformer block\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(hidden, attn_heads, hidden * 4, dropout) for _ in range(n_layers)]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, segment_info) :\n",
    "        # attention masking\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "        \n",
    "        # embedding the indexed sequence to sequence of vectors\n",
    "        x = self.embedding(x, segment_info)\n",
    "        \n",
    "        # run multiple transformer block\n",
    "        for transformer in self.transformer_blocks :\n",
    "            x = transformer.forward(x, mask)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "language_model.py\n",
    "\"\"\"\n",
    "\n",
    "class MaskedLanguageModel(nn.Module) :\n",
    "    def __init__(self, hidden, vocab_size) :\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim = -1)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        return self.softmax(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "language_model.py\n",
    "\"\"\"\n",
    "\n",
    "class NextSentencePrediction(nn.Module) :\n",
    "    \n",
    "    def __init__(self, hidden) :\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, 2)\n",
    "        self.softmaz = nn.LogSoftmax(dim = -1)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        return self.softmax(self.linear(x[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "language_model.py\n",
    "\"\"\"\n",
    "\n",
    "class BERTLTM(nn.Module) :\n",
    "    \n",
    "    def __init__(self, bert : BERT, vocab_size) :\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.next_sentence = NextSentencePrediction(self.bert.hidden)\n",
    "        self.mask_lm = MaskedLanguageModel(self.bert.hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, x, segment_label) :\n",
    "        x = self.bert(x, segment_label)\n",
    "        return self.next_sentence(x), self.mask_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "import tqdm\n",
    "\n",
    "class BERTTrainer :\n",
    "    \n",
    "    def __init__(self, bert : BERT, vocab_size : int, train_dataloader : DataLoader, test_dataloader : DataLoader = None,\n",
    "                 lr : float = 1e-4, betas = (0.9, 0.999), weight_decay : float = 0.01, warmup_steps = 10000,\n",
    "                 with_cuda : bool = True, cuda_devices = None, log_freq : int = 10) :\n",
    "        \n",
    "        \"\"\"\n",
    "        :param bert: BERT model which you want to train\n",
    "        :param vocab_size: total word vocab size\n",
    "        :param train_dataloader: train dataset data loader\n",
    "        :param test_dataloader: test dataset data loader [can be None]\n",
    "        :param lr: learning rate of optimizer\n",
    "        :param betas: Adam optimizer betas\n",
    "        :param weight_decay: Adam optimizer weight decay param\n",
    "        :param with_cuda: traning with cuda\n",
    "        :param log_freq: logging frequency of the batch iteration\n",
    "        \"\"\"\n",
    "        \n",
    "                # Setup cuda device for BERT training, argument -c, --cuda should be true\n",
    "        cuda_condition = torch.cuda.is_available() and with_cuda\n",
    "        self.device = torch.device(\"cuda:0\" if cuda_condition else \"cpu\")\n",
    "\n",
    "        # This BERT model will be saved every epoch\n",
    "        self.bert = bert\n",
    "        # Initialize the BERT Language Model, with BERT model\n",
    "        self.model = BERTLM(bert, vocab_size).to(self.device)\n",
    "\n",
    "        # Distributed GPU training if CUDA can detect more than 1 GPU\n",
    "        if with_cuda and torch.cuda.device_count() > 1:\n",
    "            print(\"Using %d GPUS for BERT\" % torch.cuda.device_count())\n",
    "            self.model = nn.DataParallel(self.model, device_ids=cuda_devices)\n",
    "\n",
    "        # Setting the train and test data loader\n",
    "        self.train_data = train_dataloader\n",
    "        self.test_data = test_dataloader\n",
    "\n",
    "        # Setting the Adam optimizer with hyper-param\n",
    "        self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        self.optim_schedule = ScheduledOptim(self.optim, self.bert.hidden, n_warmup_steps=warmup_steps)\n",
    "\n",
    "        # Using Negative Log Likelihood Loss function for predicting the masked_token\n",
    "        self.criterion = nn.NLLLoss(ignore_index=0)\n",
    "\n",
    "        self.log_freq = log_freq\n",
    "\n",
    "        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.iteration(epoch, self.train_data)\n",
    "\n",
    "    def test(self, epoch):\n",
    "        self.iteration(epoch, self.test_data, train=False)\n",
    "\n",
    "    def iteration(self, epoch, data_loader, train=True):\n",
    "        \"\"\"\n",
    "        loop over the data_loader for training or testing\n",
    "        if on train status, backward operation is activated\n",
    "        and also auto save the model every peoch\n",
    "        :param epoch: current epoch index\n",
    "        :param data_loader: torch.utils.data.DataLoader for iteration\n",
    "        :param train: boolean value of is train or test\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        str_code = \"train\" if train else \"test\"\n",
    "\n",
    "        # Setting the tqdm progress bar\n",
    "        data_iter = tqdm.tqdm(enumerate(data_loader),\n",
    "                              desc=\"EP_%s:%d\" % (str_code, epoch),\n",
    "                              total=len(data_loader),\n",
    "                              bar_format=\"{l_bar}{r_bar}\")\n",
    "\n",
    "        avg_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_element = 0\n",
    "\n",
    "        for i, data in data_iter:\n",
    "            # 0. batch_data will be sent into the device(GPU or cpu)\n",
    "            data = {key: value.to(self.device) for key, value in data.items()}\n",
    "\n",
    "            # 1. forward the next_sentence_prediction and masked_lm model\n",
    "            next_sent_output, mask_lm_output = self.model.forward(data[\"bert_input\"], data[\"segment_label\"])\n",
    "\n",
    "            # 2-1. NLL(negative log likelihood) loss of is_next classification result\n",
    "            next_loss = self.criterion(next_sent_output, data[\"is_next\"])\n",
    "\n",
    "            # 2-2. NLLLoss of predicting masked token word\n",
    "            mask_loss = self.criterion(mask_lm_output.transpose(1, 2), data[\"bert_label\"])\n",
    "\n",
    "            # 2-3. Adding next_loss and mask_loss : 3.4 Pre-training Procedure\n",
    "            loss = next_loss + mask_loss\n",
    "\n",
    "            # 3. backward and optimization only in train\n",
    "            if train:\n",
    "                self.optim_schedule.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim_schedule.step_and_update_lr()\n",
    "\n",
    "            # next sentence prediction accuracy\n",
    "            correct = next_sent_output.argmax(dim=-1).eq(data[\"is_next\"]).sum().item()\n",
    "            avg_loss += loss.item()\n",
    "            total_correct += correct\n",
    "            total_element += data[\"is_next\"].nelement()\n",
    "\n",
    "            post_fix = {\n",
    "                \"epoch\": epoch,\n",
    "                \"iter\": i,\n",
    "                \"avg_loss\": avg_loss / (i + 1),\n",
    "                \"avg_acc\": total_correct / total_element * 100,\n",
    "                \"loss\": loss.item()\n",
    "            }\n",
    "\n",
    "            if i % self.log_freq == 0:\n",
    "                data_iter.write(str(post_fix))\n",
    "\n",
    "        print(\"EP%d_%s, avg_loss=\" % (epoch, str_code), avg_loss / len(data_iter), \"total_acc=\",\n",
    "              total_correct * 100.0 / total_element)\n",
    "\n",
    "    def save(self, epoch, file_path=\"output/bert_trained.model\"):\n",
    "        \"\"\"\n",
    "        Saving the current BERT model on file_path\n",
    "        :param epoch: current epoch number\n",
    "        :param file_path: model output path which gonna be file_path+\"ep%d\" % epoch\n",
    "        :return: final_output_path\n",
    "        \"\"\"\n",
    "        output_path = file_path + \".ep%d\" % epoch\n",
    "        torch.save(self.bert.cpu(), output_path)\n",
    "        self.bert.to(self.device)\n",
    "        print(\"EP:%d Model Saved on:\" % epoch, output_path)\n",
    "        return output_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patchmix2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
