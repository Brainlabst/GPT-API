{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1BwO4-IT6ZK_k93yPIGtiCd1MIz6ABbBW","authorship_tag":"ABX9TyO0X+zuovj00Bk62qTloTZu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["%cd /content/drive/MyDrive/Lab/NLP"],"metadata":{"id":"22IIcskKQR9Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703821708645,"user_tz":-540,"elapsed":842,"user":{"displayName":"정승규","userId":"09485961021551940043"}},"outputId":"5fb9fff7-76a3-43e9-dfb8-bf523cc5b74e"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Lab/NLP\n"]}]},{"cell_type":"code","execution_count":20,"metadata":{"id":"rURwHvX7K23D","executionInfo":{"status":"ok","timestamp":1703821708646,"user_tz":-540,"elapsed":7,"user":{"displayName":"정승규","userId":"09485961021551940043"}}},"outputs":[],"source":["from torch import nn\n","import math"]},{"cell_type":"code","source":["class TokenEmbedding(nn.Embedding):\n","    \"\"\"\n","    Token Embedding using torch.nn\n","    they will dense representation of word using weighted matrix\n","    \"\"\"\n","\n","    def __init__(self, vocab_size, d_model):\n","        \"\"\"\n","        class for token embedding that included positional information\n","\n","        :param vocab_size: size of vocabulary\n","        :param d_model: dimensions of model\n","        \"\"\"\n","        super(TokenEmbedding, self).__init__(vocab_size, d_model, padding_idx=1)"],"metadata":{"id":"S0ZSYAXPsalM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PositionalEncoding(nn.Module):\n","    \"\"\"\n","    compute sinusoid encoding.\n","    \"\"\"\n","\n","    def __init__(self, d_model, max_len, device):\n","        \"\"\"\n","        constructor of sinusoid encoding class\n","\n","        :param d_model: dimension of model\n","        :param max_len: max sequence length\n","        :param device: hardware device setting\n","        \"\"\"\n","        super(PositionalEncoding, self).__init__()\n","\n","        # same size with input matrix (for adding with input matrix)\n","        self.encoding = torch.zeros(max_len, d_model, device=device)\n","        self.encoding.requires_grad = False  # we don't need to compute gradient\n","\n","        pos = torch.arange(0, max_len, device=device)\n","        pos = pos.float().unsqueeze(dim=1)\n","        # 1D => 2D unsqueeze to represent word's position\n","\n","        _2i = torch.arange(0, d_model, step=2, device=device).float()\n","        # 'i' means index of d_model (e.g. embedding size = 50, 'i' = [0,50])\n","        # \"step=2\" means 'i' multiplied with two (same with 2 * i)\n","\n","        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n","        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n","        # compute positional encoding to consider positional information of words\n","\n","    def forward(self, x):\n","        # self.encoding\n","        # [max_len = 512, d_model = 512]\n","\n","        batch_size, seq_len = x.size()\n","        # [batch_size = 128, seq_len = 30]\n","\n","        return self.encoding[:seq_len, :]\n","        # [seq_len = 30, d_model = 512]\n","        # it will add with tok_emb : [128, 30, 512]"],"metadata":{"id":"Tv7cH5q57_mj","executionInfo":{"status":"ok","timestamp":1703821708646,"user_tz":-540,"elapsed":6,"user":{"displayName":"정승규","userId":"09485961021551940043"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["class TransformerEmbedding(nn.Module):\n","    \"\"\"\n","    token embedding + positional encoding (sinusoid)\n","    positional encoding can give positional information to network\n","    \"\"\"\n","\n","    def __init__(self, vocab_size, d_model, max_len, drop_prob, device):\n","        \"\"\"\n","        class for word embedding that included positional information\n","\n","        :param vocab_size: size of vocabulary\n","        :param d_model: dimensions of model\n","        \"\"\"\n","        super(TransformerEmbedding, self).__init__()\n","        self.tok_emb = TokenEmbedding(vocab_size, d_model)\n","        self.pos_emb = PositionalEncoding(d_model, max_len, device)\n","        self.drop_out = nn.Dropout(p=drop_prob)\n","\n","    def forward(self, x):\n","        tok_emb = self.tok_emb(x)\n","        pos_emb = self.pos_emb(x)\n","        return self.drop_out(tok_emb + pos_emb)"],"metadata":{"id":"5CYdI0gX8Lst","executionInfo":{"status":"ok","timestamp":1703821708646,"user_tz":-540,"elapsed":6,"user":{"displayName":"정승규","userId":"09485961021551940043"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["class ScaleDotProductAttention(nn.Module):\n","    \"\"\"\n","    compute scale dot product attention\n","\n","    Query : given sentence that we focused on (decoder)\n","    Key : every sentence to check relationship with Qeury(encoder)\n","    Value : every sentence same with Key (encoder)\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(ScaleDotProductAttention, self).__init__()\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, q, k, v, mask=None, e=1e-12):\n","        # input is 4 dimension tensor\n","        # [batch_size, head, length, d_tensor]\n","        batch_size, head, length, d_tensor = k.size()\n","\n","        # 1. dot product Query with Key^T to compute similarity\n","        k_t = k.transpose(2, 3)  # transpose\n","        score = (q @ k_t) / math.sqrt(d_tensor)  # scaled dot product\n","\n","        # 2. apply masking (opt)\n","        if mask is not None:\n","            score = score.masked_fill(mask == 0, -10000)\n","\n","        # 3. pass them softmax to make [0, 1] range\n","        score = self.softmax(score)\n","\n","        # 4. multiply with Value\n","        v = score @ v\n","\n","        return v, score"],"metadata":{"id":"RPi6yZMo8T1J","executionInfo":{"status":"ok","timestamp":1703821708646,"user_tz":-540,"elapsed":6,"user":{"displayName":"정승규","userId":"09485961021551940043"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","\n","    def __init__(self, d_model, n_head):\n","        super(MultiHeadAttention, self).__init__()\n","        self.n_head = n_head\n","        self.attention = ScaleDotProductAttention()\n","        self.w_q = nn.Linear(d_model, d_model)\n","        self.w_k = nn.Linear(d_model, d_model)\n","        self.w_v = nn.Linear(d_model, d_model)\n","        self.w_concat = nn.Linear(d_model, d_model)\n","\n","    def forward(self, q, k, v, mask=None):\n","        # 1. dot product with weight matrices\n","        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n","\n","        # 2. split tensor by number of heads\n","        q, k, v = self.split(q), self.split(k), self.split(v)\n","\n","        # 3. do scale dot product to compute similarity\n","        out, attention = self.attention(q, k, v, mask=mask)\n","\n","        # 4. concat and pass to linear layer\n","        out = self.concat(out)\n","        out = self.w_concat(out)\n","\n","        # 5. visualize attention map\n","        # TODO : we should implement visualization\n","\n","        return out\n","\n","    def split(self, tensor):\n","        \"\"\"\n","        split tensor by number of head\n","\n","        :param tensor: [batch_size, length, d_model]\n","        :return: [batch_size, head, length, d_tensor]\n","        \"\"\"\n","        batch_size, length, d_model = tensor.size()\n","\n","        d_tensor = d_model // self.n_head\n","        tensor = tensor.view(batch_size, length, self.n_head, d_tensor).transpose(1, 2)\n","        # it is similar with group convolution (split by number of heads)\n","\n","        return tensor\n","\n","    def concat(self, tensor):\n","        \"\"\"\n","        inverse function of self.split(tensor : torch.Tensor)\n","\n","        :param tensor: [batch_size, head, length, d_tensor]\n","        :return: [batch_size, length, d_model]\n","        \"\"\"\n","        batch_size, head, length, d_tensor = tensor.size()\n","        d_model = head * d_tensor\n","\n","        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n","        return tensor"],"metadata":{"id":"6iP2aZN-8TyV","executionInfo":{"status":"ok","timestamp":1703821708646,"user_tz":-540,"elapsed":6,"user":{"displayName":"정승규","userId":"09485961021551940043"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["class LayerNorm(nn.Module):\n","    def __init__(self, d_model, eps=1e-12):\n","        super(LayerNorm, self).__init__()\n","        self.gamma = nn.Parameter(torch.ones(d_model))\n","        self.beta = nn.Parameter(torch.zeros(d_model))\n","        self.eps = eps\n","\n","    def forward(self, x):\n","        mean = x.mean(-1, keepdim=True)\n","        var = x.var(-1, unbiased=False, keepdim=True)\n","        # '-1' means last dimension.\n","\n","        out = (x - mean) / torch.sqrt(var + self.eps)\n","        out = self.gamma * out + self.beta\n","        return out"],"metadata":{"id":"_AXW1hSr8eiJ","executionInfo":{"status":"ok","timestamp":1703821708646,"user_tz":-540,"elapsed":6,"user":{"displayName":"정승규","userId":"09485961021551940043"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["class PositionwiseFeedForward(nn.Module):\n","\n","    def __init__(self, d_model, hidden, drop_prob=0.1):\n","        super(PositionwiseFeedForward, self).__init__()\n","        self.linear1 = nn.Linear(d_model, hidden)\n","        self.linear2 = nn.Linear(hidden, d_model)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(p=drop_prob)\n","\n","    def forward(self, x):\n","        x = self.linear1(x)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        x = self.linear2(x)\n","        return x"],"metadata":{"id":"fnsQcyxc8eV9","executionInfo":{"status":"ok","timestamp":1703821708646,"user_tz":-540,"elapsed":5,"user":{"displayName":"정승규","userId":"09485961021551940043"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["class EncoderLayer(nn.Module):\n","\n","    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n","        super(EncoderLayer, self).__init__()\n","        self.attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n","        self.norm1 = LayerNorm(d_model=d_model)\n","        self.dropout1 = nn.Dropout(p=drop_prob)\n","\n","        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n","        self.norm2 = LayerNorm(d_model=d_model)\n","        self.dropout2 = nn.Dropout(p=drop_prob)\n","\n","    def forward(self, x, src_mask):\n","        # 1. compute self attention\n","        _x = x\n","        x = self.attention(q=x, k=x, v=x, mask=src_mask)\n","\n","        # 2. add and norm\n","        x = self.dropout1(x)\n","        x = self.norm1(x + _x)\n","\n","        # 3. positionwise feed forward network\n","        _x = x\n","        x = self.ffn(x)\n","\n","        # 4. add and norm\n","        x = self.dropout2(x)\n","        x = self.norm2(x + _x)\n","        return x"],"metadata":{"id":"NIaBxDVU7_kC","executionInfo":{"status":"ok","timestamp":1703821708647,"user_tz":-540,"elapsed":6,"user":{"displayName":"정승규","userId":"09485961021551940043"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["class DecoderLayer(nn.Module):\n","\n","    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n","        super(DecoderLayer, self).__init__()\n","        self.self_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n","        self.norm1 = LayerNorm(d_model=d_model)\n","        self.dropout1 = nn.Dropout(p=drop_prob)\n","\n","        self.enc_dec_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n","        self.norm2 = LayerNorm(d_model=d_model)\n","        self.dropout2 = nn.Dropout(p=drop_prob)\n","\n","        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n","        self.norm3 = LayerNorm(d_model=d_model)\n","        self.dropout3 = nn.Dropout(p=drop_prob)\n","\n","    def forward(self, dec, enc, trg_mask, src_mask):\n","        # 1. compute self attention\n","        _x = dec\n","        x = self.self_attention(q=dec, k=dec, v=dec, mask=trg_mask)\n","\n","        # 2. add and norm\n","        x = self.dropout1(x)\n","        x = self.norm1(x + _x)\n","\n","        if enc is not None:\n","            # 3. compute encoder - decoder attention\n","            _x = x\n","            x = self.enc_dec_attention(q=x, k=enc, v=enc, mask=src_mask)\n","\n","            # 4. add and norm\n","            x = self.dropout2(x)\n","            x = self.norm2(x + _x)\n","\n","        # 5. positionwise feed forward network\n","        _x = x\n","        x = self.ffn(x)\n","\n","        # 6. add and norm\n","        x = self.dropout3(x)\n","        x = self.norm3(x + _x)\n","        return x"],"metadata":{"id":"CpaH1mXC7-9K","executionInfo":{"status":"ok","timestamp":1703821708647,"user_tz":-540,"elapsed":6,"user":{"displayName":"정승규","userId":"09485961021551940043"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["class Encoder(nn.Module):\n","\n","    def __init__(self, enc_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n","        super().__init__()\n","        self.emb = TransformerEmbedding(d_model=d_model,\n","                                        max_len=max_len,\n","                                        vocab_size=enc_voc_size,\n","                                        drop_prob=drop_prob,\n","                                        device=device)\n","\n","        self.layers = nn.ModuleList([EncoderLayer(d_model=d_model,\n","                                                  ffn_hidden=ffn_hidden,\n","                                                  n_head=n_head,\n","                                                  drop_prob=drop_prob)\n","                                     for _ in range(n_layers)])\n","\n","    def forward(self, x, src_mask):\n","        x = self.emb(x)\n","\n","        for layer in self.layers:\n","            x = layer(x, src_mask)\n","\n","        return x"],"metadata":{"id":"E0fw6qKP7-6n","executionInfo":{"status":"ok","timestamp":1703821708647,"user_tz":-540,"elapsed":6,"user":{"displayName":"정승규","userId":"09485961021551940043"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["class Decoder(nn.Module):\n","    def __init__(self, dec_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n","        super().__init__()\n","        self.emb = TransformerEmbedding(d_model=d_model,\n","                                        drop_prob=drop_prob,\n","                                        max_len=max_len,\n","                                        vocab_size=dec_voc_size,\n","                                        device=device)\n","\n","        self.layers = nn.ModuleList([DecoderLayer(d_model=d_model,\n","                                                  ffn_hidden=ffn_hidden,\n","                                                  n_head=n_head,\n","                                                  drop_prob=drop_prob)\n","                                     for _ in range(n_layers)])\n","\n","        self.linear = nn.Linear(d_model, dec_voc_size)\n","\n","    def forward(self, trg, enc_src, trg_mask, src_mask):\n","        trg = self.emb(trg)\n","\n","        for layer in self.layers:\n","            trg = layer(trg, enc_src, trg_mask, src_mask)\n","\n","        # pass to LM head\n","        output = self.linear(trg)\n","        return output"],"metadata":{"id":"xKlaUYeC7-4G","executionInfo":{"status":"ok","timestamp":1703821708647,"user_tz":-540,"elapsed":6,"user":{"displayName":"정승규","userId":"09485961021551940043"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","\n","class Transformer(nn.Module):\n","\n","    def __init__(self, src_pad_idx, trg_pad_idx, trg_sos_idx, enc_voc_size, dec_voc_size, d_model, n_head, max_len,\n","                 ffn_hidden, n_layers, drop_prob, device):\n","        super().__init__()\n","        self.src_pad_idx = src_pad_idx\n","        self.trg_pad_idx = trg_pad_idx\n","        self.trg_sos_idx = trg_sos_idx\n","        self.device = device\n","        self.encoder = Encoder(d_model=d_model,\n","                               n_head=n_head,\n","                               max_len=max_len,\n","                               ffn_hidden=ffn_hidden,\n","                               enc_voc_size=enc_voc_size,\n","                               drop_prob=drop_prob,\n","                               n_layers=n_layers,\n","                               device=device)\n","\n","        self.decoder = Decoder(d_model=d_model,\n","                               n_head=n_head,\n","                               max_len=max_len,\n","                               ffn_hidden=ffn_hidden,\n","                               dec_voc_size=dec_voc_size,\n","                               drop_prob=drop_prob,\n","                               n_layers=n_layers,\n","                               device=device)\n","\n","    def forward(self, src, trg):\n","        src_mask = self.make_src_mask(src)\n","        trg_mask = self.make_trg_mask(trg)\n","        enc_src = self.encoder(src, src_mask)\n","        output = self.decoder(trg, enc_src, trg_mask, src_mask)\n","        return output\n","\n","    def make_src_mask(self, src):\n","        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n","        return src_mask\n","\n","    def make_trg_mask(self, trg):\n","        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(3)\n","        trg_len = trg.shape[1]\n","        trg_sub_mask = torch.tril(torch.ones(trg_len, trg_len)).type(torch.ByteTensor).to(self.device)\n","        trg_mask = trg_pad_mask & trg_sub_mask\n","        return trg_mask"],"metadata":{"id":"NYtFzXOE7-15","executionInfo":{"status":"ok","timestamp":1703821708648,"user_tz":-540,"elapsed":6,"user":{"displayName":"정승규","userId":"09485961021551940043"}}},"execution_count":32,"outputs":[]}]}